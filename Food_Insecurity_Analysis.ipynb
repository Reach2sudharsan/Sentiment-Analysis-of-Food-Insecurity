{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-23T07:09:57.964334Z","iopub.execute_input":"2022-08-23T07:09:57.965125Z","iopub.status.idle":"2022-08-23T07:09:57.999618Z","shell.execute_reply.started":"2022-08-23T07:09:57.965084Z","shell.execute_reply":"2022-08-23T07:09:57.998426Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:25.614614Z","iopub.execute_input":"2022-08-23T18:13:25.615175Z","iopub.status.idle":"2022-08-23T18:13:26.894352Z","shell.execute_reply.started":"2022-08-23T18:13:25.615058Z","shell.execute_reply":"2022-08-23T18:13:26.893175Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:31.687745Z","iopub.execute_input":"2022-08-23T18:13:31.688402Z","iopub.status.idle":"2022-08-23T18:13:31.746766Z","shell.execute_reply.started":"2022-08-23T18:13:31.688367Z","shell.execute_reply":"2022-08-23T18:13:31.745672Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv')\nvalid = pd.read_csv('../input/imdb-dataset-sentiment-analysis-in-csv-format/Valid.csv')\ntest = pd.read_csv('../input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:33.313808Z","iopub.execute_input":"2022-08-23T18:13:33.314432Z","iopub.status.idle":"2022-08-23T18:13:34.712081Z","shell.execute_reply.started":"2022-08-23T18:13:33.314395Z","shell.execute_reply":"2022-08-23T18:13:34.711099Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset = pd.concat((train, valid, test))\ndataset.set_index(np.array([i for i in range(50000)]))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:35.576702Z","iopub.execute_input":"2022-08-23T18:13:35.577384Z","iopub.status.idle":"2022-08-23T18:13:35.609841Z","shell.execute_reply.started":"2022-08-23T18:13:35.577347Z","shell.execute_reply":"2022-08-23T18:13:35.608842Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Replacing the values to ease understanding.\ndataset['label'] = dataset['label'].replace(4,1)\n\n# Plotting the distribution for dataset.\nax = dataset.groupby('label').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax.set_xticklabels(['Negative','Positive'], rotation=0)\n\n# Storing data in lists.\ntext, sentiment = list(dataset['text']), list(dataset['label'])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:37.709277Z","iopub.execute_input":"2022-08-23T18:13:37.709640Z","iopub.status.idle":"2022-08-23T18:13:37.950304Z","shell.execute_reply.started":"2022-08-23T18:13:37.709608Z","shell.execute_reply":"2022-08-23T18:13:37.949356Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\n## Defining set containing all stopwords in english.\nstopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n             \"youve\", 'your', 'yours', 'yourself', 'yourselves']","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:39.904805Z","iopub.execute_input":"2022-08-23T18:13:39.905588Z","iopub.status.idle":"2022-08-23T18:13:39.917880Z","shell.execute_reply.started":"2022-08-23T18:13:39.905551Z","shell.execute_reply":"2022-08-23T18:13:39.916363Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:41.823197Z","iopub.execute_input":"2022-08-23T18:13:41.823563Z","iopub.status.idle":"2022-08-23T18:13:41.829078Z","shell.execute_reply.started":"2022-08-23T18:13:41.823532Z","shell.execute_reply":"2022-08-23T18:13:41.827630Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer\n    wordLemm = WordNetLemmatizer()\n    \n    # Regex patterns\n    urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,' URL',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,' USER', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            # If word not in stopwordlist\n            if len(word)>1:\n                \n                word = wordLemm.lemmatize(word)\n            \n                tweetwords += (word+' ')\n        processedText.append(tweetwords)\n        \n    return processedText","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:43.406640Z","iopub.execute_input":"2022-08-23T18:13:43.407715Z","iopub.status.idle":"2022-08-23T18:13:43.416742Z","shell.execute_reply.started":"2022-08-23T18:13:43.407680Z","shell.execute_reply":"2022-08-23T18:13:43.415530Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import time\nt = time.time()\nprocessedtext = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:13:45.320894Z","iopub.execute_input":"2022-08-23T18:13:45.321584Z","iopub.status.idle":"2022-08-23T18:14:51.131939Z","shell.execute_reply.started":"2022-08-23T18:13:45.321543Z","shell.execute_reply":"2022-08-23T18:14:51.130919Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n                                                    test_size = 0.1, random_state = 0)\nprint(f'Data Split done.')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:02.929477Z","iopub.execute_input":"2022-08-23T18:15:02.929848Z","iopub.status.idle":"2022-08-23T18:15:02.959669Z","shell.execute_reply.started":"2022-08-23T18:15:02.929818Z","shell.execute_reply":"2022-08-23T18:15:02.958606Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"np.array(X_train).shape","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:04.917672Z","iopub.execute_input":"2022-08-23T18:15:04.918055Z","iopub.status.idle":"2022-08-23T18:15:06.587848Z","shell.execute_reply.started":"2022-08-23T18:15:04.918016Z","shell.execute_reply":"2022-08-23T18:15:06.586872Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud\nfrom keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:11.973754Z","iopub.execute_input":"2022-08-23T18:15:11.974356Z","iopub.status.idle":"2022-08-23T18:15:16.874140Z","shell.execute_reply.started":"2022-08-23T18:15:11.974316Z","shell.execute_reply":"2022-08-23T18:15:16.872956Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def prepare_data(X_train, X_test):\n    #Tokenize the sentences\n    tokenizer = Tokenizer()\n    #preparing vocabulary\n    tokenizer.fit_on_texts(processedtext)\n    #converting text into integer sequences\n    X_train = tokenizer.texts_to_sequences(X_train)\n    X_test = tokenizer.texts_to_sequences(X_test)\n    #padding to prepare sequences of same length\n    X_train=pad_sequences(X_train,maxlen=120)\n    X_test=pad_sequences(X_test,maxlen=120)\n\n    size_of_vocabulary = len(tokenizer.word_index)+1\n    print(\"Vocabulary Size: \" + str(size_of_vocabulary))\n\n    return X_train, X_test, size_of_vocabulary","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:16.876654Z","iopub.execute_input":"2022-08-23T18:15:16.877934Z","iopub.status.idle":"2022-08-23T18:15:16.885536Z","shell.execute_reply.started":"2022-08-23T18:15:16.877878Z","shell.execute_reply":"2022-08-23T18:15:16.883960Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, size_of_vocabulary = prepare_data(X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:18.588768Z","iopub.execute_input":"2022-08-23T18:15:18.589452Z","iopub.status.idle":"2022-08-23T18:15:31.725069Z","shell.execute_reply.started":"2022-08-23T18:15:18.589414Z","shell.execute_reply":"2022-08-23T18:15:31.723964Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from keras.layers import *\nfrom keras.models import *\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:43.203702Z","iopub.execute_input":"2022-08-23T18:15:43.204811Z","iopub.status.idle":"2022-08-23T18:15:43.210386Z","shell.execute_reply.started":"2022-08-23T18:15:43.204765Z","shell.execute_reply":"2022-08-23T18:15:43.209244Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def build_BiLSTM(size_of_vocabulary):\n    model = Sequential()\n    #embedding layer\n    model.add(Embedding(size_of_vocabulary,128,input_length=120))\n    #lstm layer\n    model.add(Bidirectional(LSTM(64,return_sequences=True,dropout=0.2)))\n    #Global Maxpooling\n    model.add(GlobalMaxPooling1D())\n    #Dense Layer\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(0.05))\n    model.add(Dense(1,activation='sigmoid'))\n    #Add loss function, metrics, optimizer\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    #Adding callbacks\n    es = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=3)\n    mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1)\n    #summary\n    return model, es, mc\n\ndef fit(model, X_train, X_test, y_train, y_test, es, mc):\n    history = model.fit(X_train,y_train,batch_size=128,epochs=4, \n    validation_data=(X_test,y_test),verbose=1,callbacks=[es,mc])\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:18:49.211849Z","iopub.execute_input":"2022-08-23T18:18:49.212300Z","iopub.status.idle":"2022-08-23T18:18:49.222621Z","shell.execute_reply.started":"2022-08-23T18:18:49.212252Z","shell.execute_reply":"2022-08-23T18:18:49.221302Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"X = np.concatenate((X_train, X_test))\ny = np.concatenate((y_train, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:15:58.145514Z","iopub.execute_input":"2022-08-23T18:15:58.145966Z","iopub.status.idle":"2022-08-23T18:15:58.161678Z","shell.execute_reply.started":"2022-08-23T18:15:58.145923Z","shell.execute_reply":"2022-08-23T18:15:58.160674Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model, es, mc = build_BiLSTM(size_of_vocabulary)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:18:51.765810Z","iopub.execute_input":"2022-08-23T18:18:51.766435Z","iopub.status.idle":"2022-08-23T18:19:03.758622Z","shell.execute_reply.started":"2022-08-23T18:18:51.766401Z","shell.execute_reply":"2022-08-23T18:19:03.756655Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nn_split=5\nhistories = []\ni = 1\n\nfor train_index,test_index in KFold(n_split).split(X):\n    print(\"Fold {}\".format(i))\n    X_train,X_test=X[train_index],X[test_index]\n    y_train,y_test=y[train_index],y[test_index]\n\n    model, es, mc=build_BiLSTM(size_of_vocabulary)\n    history=model.fit(X_train, y_train,batch_size=128,epochs=4)\n    model.save('model_fold_{}.h5'.format(i))\n    histories.append(history)\n    \n    print('Model evaluation ',model.evaluate(X_test,y_test))\n    print(\"\")\n    i = i + 1","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:19:07.582615Z","iopub.execute_input":"2022-08-23T18:19:07.583303Z","iopub.status.idle":"2022-08-23T18:22:08.413817Z","shell.execute_reply.started":"2022-08-23T18:19:07.583266Z","shell.execute_reply":"2022-08-23T18:22:08.412853Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"food_desert_data = pd.read_csv('../input/food-insecurity-dataset/Food_Deserts_and_Insecurity.csv')\nfood_desert_data","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:23:54.720387Z","iopub.execute_input":"2022-08-23T18:23:54.720786Z","iopub.status.idle":"2022-08-23T18:23:54.775884Z","shell.execute_reply.started":"2022-08-23T18:23:54.720754Z","shell.execute_reply":"2022-08-23T18:23:54.774910Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"location = np.array(food_desert_data['user_location'])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:26:30.606516Z","iopub.execute_input":"2022-08-23T18:26:30.607109Z","iopub.status.idle":"2022-08-23T18:26:30.612575Z","shell.execute_reply.started":"2022-08-23T18:26:30.607065Z","shell.execute_reply":"2022-08-23T18:26:30.611213Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import time\nt = time.time()\ntext = preprocess(food_desert_data['text'])\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:26:32.214763Z","iopub.execute_input":"2022-08-23T18:26:32.215458Z","iopub.status.idle":"2022-08-23T18:26:32.678045Z","shell.execute_reply.started":"2022-08-23T18:26:32.215420Z","shell.execute_reply":"2022-08-23T18:26:32.676966Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#Tokenize the sentences\ntokenizer = Tokenizer()\n#preparing vocabulary\ntokenizer.fit_on_texts(text)\n#converting text into integer sequences\nprocessedtext = tokenizer.texts_to_sequences(text)\n#padding to prepare sequences of same length\nprocessedtext=pad_sequences(processedtext,maxlen=120)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:26:48.903107Z","iopub.execute_input":"2022-08-23T18:26:48.903467Z","iopub.status.idle":"2022-08-23T18:26:49.060170Z","shell.execute_reply.started":"2022-08-23T18:26:48.903436Z","shell.execute_reply":"2022-08-23T18:26:49.059236Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:27:43.022567Z","iopub.execute_input":"2022-08-23T18:27:43.022961Z","iopub.status.idle":"2022-08-23T18:27:43.028995Z","shell.execute_reply.started":"2022-08-23T18:27:43.022926Z","shell.execute_reply":"2022-08-23T18:27:43.027785Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model = load_model('./model_fold_1.h5')\nsentiment = model.predict(processedtext)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:28:44.911592Z","iopub.execute_input":"2022-08-23T18:28:44.911972Z","iopub.status.idle":"2022-08-23T18:28:46.756593Z","shell.execute_reply.started":"2022-08-23T18:28:44.911938Z","shell.execute_reply":"2022-08-23T18:28:46.755558Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sentiment_rounded = np.array(sentiment).reshape(len(sentiment),)\nfor i in range(len(sentiment_rounded)):\n    sentiment_rounded[i] = int(round(sentiment_rounded[i]))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:29:19.645542Z","iopub.execute_input":"2022-08-23T18:29:19.645926Z","iopub.status.idle":"2022-08-23T18:29:19.657944Z","shell.execute_reply.started":"2022-08-23T18:29:19.645875Z","shell.execute_reply":"2022-08-23T18:29:19.656919Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, np.array(y_test))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:59:55.957541Z","iopub.execute_input":"2022-08-23T07:59:55.958239Z","iopub.status.idle":"2022-08-23T08:00:37.860728Z","shell.execute_reply.started":"2022-08-23T07:59:55.958203Z","shell.execute_reply":"2022-08-23T08:00:37.859471Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"food = pd.DataFrame({\"text\": np.array(text).reshape(len(text),), \"label\": sentiment_rounded}, index=np.array([i for i in range(len(text))]))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:30:58.765656Z","iopub.execute_input":"2022-08-23T18:30:58.766160Z","iopub.status.idle":"2022-08-23T18:30:58.777870Z","shell.execute_reply.started":"2022-08-23T18:30:58.766127Z","shell.execute_reply":"2022-08-23T18:30:58.776621Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud,ImageColorGenerator\nfrom PIL import Image\nimport urllib\nimport requests\nstr().find","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:31:00.935147Z","iopub.execute_input":"2022-08-23T18:31:00.935755Z","iopub.status.idle":"2022-08-23T18:31:00.942889Z","shell.execute_reply.started":"2022-08-23T18:31:00.935714Z","shell.execute_reply":"2022-08-23T18:31:00.941747Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"all_words_positive = ' '.join(text for text in food['text'][food['label']==1])\n\n# combining the image with the dataset\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n\n# We use the ImageColorGenerator library from Wordcloud \n# Here we take the color of the image and impose it over our wordcloud\nimage_colors = ImageColorGenerator(Mask)\n\n# Now we use the WordCloud function from the wordcloud library \nwc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_positive)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:34:58.987630Z","iopub.execute_input":"2022-08-23T19:34:58.988660Z","iopub.status.idle":"2022-08-23T19:35:03.036519Z","shell.execute_reply.started":"2022-08-23T19:34:58.988608Z","shell.execute_reply":"2022-08-23T19:35:03.035186Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Size of the image generated \nplt.figure(figsize=(10,20))\n\n# Here we recolor the words from the dataset to the image's color\n# recolor just recolors the default colors to the image's blue color\n# interpolation is used to smooth the image generated \nplt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\nplt.title('Positive Review Common Text')\nplt.axis('off')\nplt.savefig('Positive_Review_Common_Text.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:35:04.154047Z","iopub.execute_input":"2022-08-23T19:35:04.154803Z","iopub.status.idle":"2022-08-23T19:35:04.884538Z","shell.execute_reply.started":"2022-08-23T19:35:04.154755Z","shell.execute_reply":"2022-08-23T19:35:04.882948Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"all_words_negative = ' '.join(text for text in food['text'][food['label']==0])\n\n# combining the image with the dataset\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n\n# We use the ImageColorGenerator library from Wordcloud \n# Here we take the color of the image and impose it over our wordcloud\nimage_colors = ImageColorGenerator(Mask)\n\n# Now we use the WordCloud function from the wordcloud library \nwc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_negative)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:35:08.494920Z","iopub.execute_input":"2022-08-23T19:35:08.495925Z","iopub.status.idle":"2022-08-23T19:35:10.769710Z","shell.execute_reply.started":"2022-08-23T19:35:08.495855Z","shell.execute_reply":"2022-08-23T19:35:10.768668Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Size of the image generated \nplt.figure(figsize=(10,20))\n\n# Here we recolor the words from the dataset to the image's color\n# recolor just recolors the default colors to the image's blue color\n# interpolation is used to smooth the image generated \nplt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\nplt.title('Negative Review Common Text')\nplt.axis('off')\nplt.savefig('Negative_Review_Common_Text.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:35:10.774984Z","iopub.execute_input":"2022-08-23T19:35:10.777238Z","iopub.status.idle":"2022-08-23T19:35:11.793098Z","shell.execute_reply.started":"2022-08-23T19:35:10.777199Z","shell.execute_reply":"2022-08-23T19:35:11.791821Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"def view_common_words(train):\n    pos_freq = FreqDist(' '.join(train[train['label'] == 1].text).split(' '))\n    plt.figure(figsize=(20,6))\n    pos_freq.plot(50,cumulative=False,title='Positive Review Common Text', color='blue')\n    plt.gcf()\n    plt.savefig('Positive_Common_Text_Graph.png')\n    plt.show()\n\n    neg_freq = FreqDist(' '.join(train[train['label'] == 0].text).split(' '))\n    plt.figure(figsize=(20,6))\n    neg_freq.plot(50,cumulative=False,title='Negative Review Common Text',color='red')\n    plt.gcf()\n    plt.savefig('Negative_Common_Text_Graph.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:39:55.088694Z","iopub.execute_input":"2022-08-23T19:39:55.089077Z","iopub.status.idle":"2022-08-23T19:39:55.096477Z","shell.execute_reply.started":"2022-08-23T19:39:55.089045Z","shell.execute_reply":"2022-08-23T19:39:55.095277Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"view_common_words(food)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:39:56.478244Z","iopub.execute_input":"2022-08-23T19:39:56.478611Z","iopub.status.idle":"2022-08-23T19:39:57.408143Z","shell.execute_reply.started":"2022-08-23T19:39:56.478580Z","shell.execute_reply":"2022-08-23T19:39:57.406973Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"food['location'] = location","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:34:52.482508Z","iopub.execute_input":"2022-08-23T18:34:52.483148Z","iopub.status.idle":"2022-08-23T18:34:52.491384Z","shell.execute_reply.started":"2022-08-23T18:34:52.483092Z","shell.execute_reply":"2022-08-23T18:34:52.490179Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"food","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:34:58.368129Z","iopub.execute_input":"2022-08-23T18:34:58.368787Z","iopub.status.idle":"2022-08-23T18:34:58.385023Z","shell.execute_reply.started":"2022-08-23T18:34:58.368754Z","shell.execute_reply":"2022-08-23T18:34:58.383843Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"p_food = food[food['label'] == 1.0]\nn_food = food[food['label'] == 0.0]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:35:13.209403Z","iopub.execute_input":"2022-08-23T18:35:13.210012Z","iopub.status.idle":"2022-08-23T18:35:13.218611Z","shell.execute_reply.started":"2022-08-23T18:35:13.209976Z","shell.execute_reply":"2022-08-23T18:35:13.217420Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"p_food","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:35:20.065428Z","iopub.execute_input":"2022-08-23T18:35:20.065794Z","iopub.status.idle":"2022-08-23T18:35:20.083337Z","shell.execute_reply.started":"2022-08-23T18:35:20.065763Z","shell.execute_reply":"2022-08-23T18:35:20.081843Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"n_food","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:35:25.578520Z","iopub.execute_input":"2022-08-23T18:35:25.578957Z","iopub.status.idle":"2022-08-23T18:35:25.600477Z","shell.execute_reply.started":"2022-08-23T18:35:25.578919Z","shell.execute_reply":"2022-08-23T18:35:25.599426Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"p_locations = np.array(p_food['location'])\np_text = np.array(p_food['text'])\np_loc_clean = []\np_text_clean = []\nfor i in range(len(p_locations)):\n    if type(p_locations[i]) == str:\n        p_loc_clean.append(p_locations[i])\n        p_text_clean.append(p_text[i])\n\nn_locations = np.array(n_food['location'])\nn_text = np.array(n_food['text'])\nn_loc_clean = []\nn_text_clean = []\nfor i in range(len(n_locations)):\n    if type(n_locations[i]) == str:\n        n_loc_clean.append(n_locations[i])\n        n_text_clean.append(n_text[i])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:35:32.730723Z","iopub.execute_input":"2022-08-23T18:35:32.731195Z","iopub.status.idle":"2022-08-23T18:35:32.749932Z","shell.execute_reply.started":"2022-08-23T18:35:32.731154Z","shell.execute_reply":"2022-08-23T18:35:32.748910Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"p_food = pd.DataFrame({\"text\":p_text_clean, \"location\":p_loc_clean})\np_food","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:35:39.683838Z","iopub.execute_input":"2022-08-23T18:35:39.684217Z","iopub.status.idle":"2022-08-23T18:35:39.700532Z","shell.execute_reply.started":"2022-08-23T18:35:39.684185Z","shell.execute_reply":"2022-08-23T18:35:39.699250Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"n_food = pd.DataFrame({\"text\":n_text_clean, \"location\":n_loc_clean})\nn_food","metadata":{"execution":{"iopub.status.busy":"2022-08-23T18:35:47.524027Z","iopub.execute_input":"2022-08-23T18:35:47.527021Z","iopub.status.idle":"2022-08-23T18:35:47.548789Z","shell.execute_reply.started":"2022-08-23T18:35:47.526977Z","shell.execute_reply":"2022-08-23T18:35:47.547950Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"p_loc = ' '.join(text for text in p_food['location'])\n\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\nimage_colors = ImageColorGenerator(Mask)\nwc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(p_loc)\n\nplt.figure(figsize=(10,20))\nplt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\nplt.title('Positive Locations')\nplt.axis('off')\nplt.savefig('Positive_Locations.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:52:50.308439Z","iopub.execute_input":"2022-08-23T19:52:50.309063Z","iopub.status.idle":"2022-08-23T19:52:53.292469Z","shell.execute_reply.started":"2022-08-23T19:52:50.309026Z","shell.execute_reply":"2022-08-23T19:52:53.291487Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"n_loc = ' '.join(text for text in n_food['location'])\n\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\nimage_colors = ImageColorGenerator(Mask)\nwc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(n_loc)\n\nplt.figure(figsize=(10,20))\nplt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\nplt.title('Negative Locations')\nplt.axis('off')\nplt.savefig('Negative_Locations.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T19:53:07.752805Z","iopub.execute_input":"2022-08-23T19:53:07.753419Z","iopub.status.idle":"2022-08-23T19:53:10.582916Z","shell.execute_reply.started":"2022-08-23T19:53:07.753382Z","shell.execute_reply":"2022-08-23T19:53:10.581867Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}